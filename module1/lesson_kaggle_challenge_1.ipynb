{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QX0jgn6FTo2I"
   },
   "source": [
    "Lambda School Data Science, Unit 2: Predictive Modeling\n",
    "\n",
    "# Kaggle Challenge, Module 1\n",
    "\n",
    "\n",
    "#### Objectives\n",
    "- clean outliers, impute missing values\n",
    "- use scikit-learn pipelines\n",
    "- use scikit-learn for decision trees\n",
    "- understand why decision trees are useful to model non-linear, non-monotonic relationships and feature interactions\n",
    "- get and interpret feature importances of a tree-based model\n",
    "\n",
    "#### Links\n",
    "\n",
    "- A Visual Introduction to Machine Learning\n",
    "  - [Part 1: A Decision Tree](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "  - [Part 2: Bias and Variance](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)\n",
    "- [Decision Trees: Advantages & Disadvantages](https://christophm.github.io/interpretable-ml-book/tree.html#advantages-2)\n",
    "- [How a Russian mathematician constructed a decision tree — by hand — to solve a medical problem](http://fastml.com/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/)\n",
    "- [How decision trees work](https://brohrer.github.io/how_decision_trees_work.html)\n",
    "- [Let’s Write a Decision Tree Classifier from Scratch](https://www.youtube.com/watch?v=LDRbO9a6XPU) — _Don’t worry about understanding the code, just get introduced to the concepts. This 10 minute video has excellent diagrams and explanations._\n",
    "- [Random Forests for Complete Beginners: The definitive guide to Random Forests and Decision Trees](https://victorzhou.com/blog/intro-to-random-forests/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u14XI47bFZTk"
   },
   "source": [
    "### Setup\n",
    "\n",
    "#### If you're using [Anaconda](https://www.anaconda.com/distribution/) locally\n",
    "\n",
    "Install required Python packages, if you haven't already:\n",
    "\n",
    "- [graphviz](https://anaconda.org/anaconda/python-graphviz)\n",
    "- [category_encoders](http://contrib.scikit-learn.org/categorical-encoding/), version >= 2.0\n",
    "- [Plotly](https://plot.ly/python/getting-started/), version >= 4.0\n",
    "\n",
    "```\n",
    "conda install python-graphviz\n",
    "```\n",
    "\n",
    "```\n",
    "conda install -c conda-forge category_encoders plotly\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OzeMxiloFV2y"
   },
   "outputs": [],
   "source": [
    "# If you're in Colab...\n",
    "import os, sys\n",
    "in_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if in_colab:\n",
    "    # Install required python packages:\n",
    "    # category_encoders, version >= 2.0\n",
    "    # pandas-profiling, version >= 2.0\n",
    "    # plotly, version >= 4.0\n",
    "    !pip install --upgrade category_encoders pandas-profiling plotly\n",
    "    \n",
    "    # Pull files from Github repo\n",
    "    os.chdir('/content')\n",
    "    !git init .\n",
    "    !git remote add origin https://github.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge.git\n",
    "    !git pull origin master\n",
    "    \n",
    "    # Change into directory for module\n",
    "    os.chdir('module1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2aRilw4DPi8"
   },
   "outputs": [],
   "source": [
    "# Use this function later\n",
    "\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def pred_heatmap(model, X, features, class_index=-1, title='', num=100):\n",
    "    \"\"\"\n",
    "    Visualize predicted probabilities, for classifier fit on 2 numeric features\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : scikit-learn classifier, already fit\n",
    "    X : pandas dataframe, which was used to fit model\n",
    "    features : list of strings, column names of the 2 numeric features\n",
    "    class_index : integer, index of class label\n",
    "    title : string, title of plot\n",
    "    num : int, number of grid points for each feature\n",
    "    \"\"\"\n",
    "    feature1, feature2 = features\n",
    "    min1, max1 = X[feature1].min(), X[feature1].max()\n",
    "    min2, max2 = X[feature2].min(), X[feature2].max()\n",
    "    x1 = np.linspace(min1, max1, num)\n",
    "    x2 = np.linspace(max2, min2, num)\n",
    "    combos = list(itertools.product(x1, x2))\n",
    "    y_pred_proba = model.predict_proba(combos)[:, class_index]\n",
    "    pred_grid = y_pred_proba.reshape(num, num).T\n",
    "    table = pd.DataFrame(pred_grid, columns=x1, index=x2)\n",
    "    plot_every_n_ticks = int(floor(num/4))\n",
    "    sns.heatmap(table, xticklabels=plot_every_n_ticks, yticklabels=plot_every_n_ticks)\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UFGmW4ijn4YN"
   },
   "source": [
    "## Clean outliers, impute missing values (example solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dg8hVHaJTldG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.merge(pd.read_csv('../data/tanzania/train_features.csv'), \n",
    "                 pd.read_csv('../data/tanzania/train_labels.csv'))\n",
    "test = pd.read_csv('../data/tanzania/test_features.csv')\n",
    "sample_submission = pd.read_csv('../data/tanzania/sample_submission.csv')\n",
    "\n",
    "# Split train into train & val\n",
    "train, val = train_test_split(train, train_size=0.80, test_size=0.20, \n",
    "                              stratify=train['status_group'], random_state=42)\n",
    "\n",
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DeIQ1lDmqAHY"
   },
   "source": [
    "Some of the locations are at [\"Null Island\"](https://en.wikipedia.org/wiki/Null_Island) instead of Tanzania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nOetSBQIHX3I"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.scatter(train, x='longitude', y='latitude', color='status_group', opacity=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aP_SmUYzbUuP"
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V-OOtTMzqkhM"
   },
   "source": [
    "#### Define a function to wrangle train, validate, and test sets in the same way.\n",
    "\n",
    "Fix the location, and do more data cleaning and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QwOx78FgHxHp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def wrangle(X):\n",
    "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
    "    \n",
    "    # Prevent SettingWithCopyWarning\n",
    "    X = X.copy()\n",
    "    \n",
    "    # About 3% of the time, latitude has small values near zero,\n",
    "    # outside Tanzania, so we'll treat these values like zero.\n",
    "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
    "    \n",
    "    # When columns have zeros and shouldn't, they are like null values.\n",
    "    # So we will replace the zeros with nulls, and impute missing values later.\n",
    "    cols_with_zeros = ['longitude', 'latitude']\n",
    "    for col in cols_with_zeros:\n",
    "        X[col] = X[col].replace(0, np.nan)\n",
    "            \n",
    "    # quantity & quantity_group are duplicates, so drop one\n",
    "    X = X.drop(columns='quantity_group')\n",
    "    \n",
    "    # return the wrangled dataframe\n",
    "    return X\n",
    "\n",
    "\n",
    "train = wrangle(train)\n",
    "val = wrangle(val)\n",
    "test = wrangle(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMqlStuWqwde"
   },
   "source": [
    "Now the locations look better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dbdljEmCVpf0"
   },
   "outputs": [],
   "source": [
    "# https://plot.ly/python/mapbox-layers/#base-maps-in-layoutmapboxstyle\n",
    "fig = px.scatter_mapbox(train, lat='latitude', lon='longitude', color='status_group', opacity=0.1)\n",
    "fig.update_layout(mapbox_style='stamen-terrain')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ypjt052Lrmgn"
   },
   "source": [
    "#### Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "666FUbXOXgL8"
   },
   "outputs": [],
   "source": [
    "# The status_group column is the target\n",
    "target = 'status_group'\n",
    "\n",
    "# Get a dataframe with all train columns except the target & id\n",
    "train_features = train.drop(columns=[target, 'id'])\n",
    "\n",
    "# Get a list of the numeric features\n",
    "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# Get a series with the cardinality of the nonnumeric features\n",
    "cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
    "\n",
    "# Get a list of all categorical features with cardinality <= 50\n",
    "categorical_features = cardinality[cardinality <= 50].index.tolist()\n",
    "\n",
    "# Combine the lists \n",
    "features = numeric_features + categorical_features\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nWGAs5BWb1t"
   },
   "outputs": [],
   "source": [
    "# Arrange data into X features matrix and y target vector \n",
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "X_val = val[features]\n",
    "y_val = val[target]\n",
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "moC9WikFrqJV"
   },
   "source": [
    "## Use scikit-learn pipelines\n",
    "\n",
    "We can combine steps with pipelines: Encode, Impute, Scale, Fit, Predict!\n",
    "\n",
    "[The Scikit-Learn User Guide explains why pipelines are useful](https://scikit-learn.org/stable/modules/compose.html), and demonstrates how to use them.\n",
    "\n",
    "> Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\n",
    "> - **Convenience and encapsulation.** You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "> - **Joint parameter selection.** You can grid search over parameters of all estimators in the pipeline at once.\n",
    "> - **Safety.** Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n",
    "\n",
    "Here's the documentation for each step in this pipeline:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html\n",
    "- https://contrib.scikit-learn.org/categorical-encoding/onehot.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rUUq8Zt5X1bD"
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OneHotEncoder(use_cat_names=True), \n",
    "    SimpleImputer(), \n",
    "    StandardScaler(), \n",
    "    LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000)\n",
    ")\n",
    "\n",
    "# Fit on train\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Score on val\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
    "\n",
    "# Predict on test\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "# Write submission csv file\n",
    "submission = sample_submission.copy()\n",
    "submission['status_group'] = y_pred\n",
    "submission.to_csv('submission-02.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7EQOYZtqr00F"
   },
   "source": [
    "#### Get and plot coefficients\n",
    "\n",
    "This is slightly harder when using pipelines.\n",
    "\n",
    "The pipeline doesn't have a `.coef_` attribute. But the model inside the pipeline does. \n",
    "\n",
    "So, here's [how to access steps inside a pipeline](https://scikit-learn.org/stable/modules/compose.html#accessing-steps):\n",
    "\n",
    "> Pipeline’s `named_steps` attribute allows accessing steps by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFlfjFN1gyjI"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = pipeline.named_steps['logisticregression']\n",
    "encoder = pipeline.named_steps['onehotencoder']\n",
    "encoded_columns = encoder.fit_transform(X_train).columns\n",
    "coefficients = pd.Series(model.coef_[0], encoded_columns)\n",
    "plt.figure(figsize=(10,30))\n",
    "coefficients.sort_values().plot.barh(color='grey');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kMW9JbD6wZ28"
   },
   "source": [
    "## Use scikit-learn for decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0s7t37euwd41"
   },
   "source": [
    "### Compare a Logistic Regression with 2 features, longitude & latitude ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44TUfQpeo64d"
   },
   "outputs": [],
   "source": [
    "train_location = X_train[['longitude', 'latitude']].copy()\n",
    "val_location = X_val[['longitude', 'latitude']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_xIa_1Kozfl"
   },
   "outputs": [],
   "source": [
    "# With just long & lat, a Logistic Regression can't beat the majority classifier baseline\n",
    "\n",
    "lr = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    LogisticRegression(solver='lbfgs', multi_class='auto', n_jobs=-1)\n",
    ")\n",
    "\n",
    "# lr = LogisticRegression(solver='lbfgs', multi_class='auto', n_jobs=-1)\n",
    "lr.fit(train_location, y_train)\n",
    "print('Logistic Regression:')\n",
    "print('Train Accuracy', lr.score(train_location, y_train))\n",
    "print('Validation Accuracy', lr.score(val_location, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yD31msBkwlVQ"
   },
   "source": [
    "### ... versus a Decision Tree Classifier with 2 features, longitude & latitude\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "We will start with default parameters, including:\n",
    "- `max_depth=None`\n",
    "- `min_samples_leaf=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RbY4s-vjiMw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZYdLLQoMcXp"
   },
   "source": [
    "### ... versus a less complex Decision Tree Classifier \n",
    "\n",
    "We can use the `min_samples_leaf` parameter to reduce model complexity.\n",
    "\n",
    "It's explained in [scikit-learn docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "Also in [A Visual Introduction to Machine Learning, Part 2](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/):\n",
    "\n",
    "> Models can be adjusted to change the way they fit the data. These 'settings' are called [hyper]parameters. An example of a decision-tree [hyper]parameter is the _minimum node size_, which regulates the creation of new splits. A node will not split if the number of data points it contains is below the minimum node size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pvAwrnXtnaZY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFvsu2onNXw7"
   },
   "source": [
    "### Visualize the logistic regression predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3yeINd4f8bRF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lJZSMvW7Nd97"
   },
   "source": [
    "### Visualize the decision tree predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJ-QtMDK-uX2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdjg7U_jNlJG"
   },
   "source": [
    "### How does a tree grow? Branch by branch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_Jhv0_VId9B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KkV4TjcNAPzp"
   },
   "source": [
    "## Understand why decision trees are useful to model non-linear, non-monotonic relationships and feature interactions\n",
    "\n",
    "#### What does _(non)monotonic_ mean?!?!\n",
    "- See Figures 1-3 in Wikipedia's article, [Monotonic function](https://en.wikipedia.org/wiki/Monotonic_function)\n",
    "- See [World Population Growth, 1700-2010](https://ourworldindata.org/world-population-growth-past-future). World Population is non-linear and monotonic. Annual growth rate is non-linear and non-monotonic.\n",
    "- See [Accidents per Mile Driven, by Driver Age](http://howwedrive.com/2009/02/20/whats-the-real-risk-of-older-drivers/). This is non-linear and non-monotonic.\n",
    "\n",
    "#### What does _feature interactions_ mean?!?!\n",
    "- See the explanation in [_Interpretable Machine Learning_, Chapter 5.4.1, Feature Interaction](https://christophm.github.io/interpretable-ml-book/interaction.html#feature-interaction).\n",
    "- See the exploration in this notebook, under the heading ***Interlude #2: Simple housing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5zlKegIZBbGr"
   },
   "source": [
    "### Interlude #1: predicting golf putts\n",
    "(1 feature, non-linear, regression)\n",
    "\n",
    "https://statmodeling.stat.columbia.edu/2008/12/04/the_golf_puttin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GjvPHk-FBayo"
   },
   "outputs": [],
   "source": [
    "columns = ['distance', 'tries', 'successes']\n",
    "data = [[2, 1443, 1346],\n",
    "        [3, 694, 577],\n",
    "        [4, 455, 337],\n",
    "        [5, 353, 208],\n",
    "        [6, 272, 149],\n",
    "        [7, 256, 136],\n",
    "        [8, 240, 111],\n",
    "        [9, 217, 69],\n",
    "        [10, 200, 67],\n",
    "        [11, 237, 75],\n",
    "        [12, 202, 52],\n",
    "        [13, 192, 46],\n",
    "        [14, 174, 54],\n",
    "        [15, 167, 28],\n",
    "        [16, 201, 27],\n",
    "        [17, 195, 31],\n",
    "        [18, 191, 33],\n",
    "        [19, 147, 20],\n",
    "        [20, 152, 24]]\n",
    "\n",
    "putts = pd.DataFrame(columns=columns, data=data)\n",
    "putts['rate of success'] = putts['successes'] / putts['tries']\n",
    "putts.plot('distance', 'rate of success', kind='scatter', title='Golf Putts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tr-83jOEBsxK"
   },
   "source": [
    "#### Compare Linear Regression ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x988kCamBpbn"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "putts_X = putts[['distance']]\n",
    "putts_y = putts['rate of success']\n",
    "lr = LinearRegression()\n",
    "lr.fit(putts_X, putts_y)\n",
    "print('R^2 Score', lr.score(putts_X, putts_y))\n",
    "ax = putts.plot('distance', 'rate of success', kind='scatter', title='Golf Putts')\n",
    "ax.plot(putts_X, lr.predict(putts_X));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MjLGrZjfFGD-"
   },
   "source": [
    "#### ... versus a Decision Tree Regressor\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EpFIetrsB2yc"
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from ipywidgets import interact\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "\n",
    "def viztree(decision_tree, feature_names):\n",
    "    dot_data = export_graphviz(decision_tree, out_file=None, feature_names=feature_names, \n",
    "                               filled=True, rounded=True)   \n",
    "    return graphviz.Source(dot_data)\n",
    "\n",
    "def putts_tree(max_depth=1):\n",
    "    tree = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    tree.fit(putts_X, putts_y)\n",
    "    print('R^2 Score', tree.score(putts_X, putts_y))\n",
    "    ax = putts.plot('distance', 'rate of success', kind='scatter', title='Golf Putts')\n",
    "    ax.step(putts_X, tree.predict(putts_X), where='mid')\n",
    "    plt.show()\n",
    "    display(viztree(tree, feature_names=['distance']))\n",
    "\n",
    "interact(putts_tree, max_depth=(1,6,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WBQJB0PVFtIE"
   },
   "source": [
    "### Interlude #2: Simple housing \n",
    "(2 features, regression)\n",
    "\n",
    "https://christophm.github.io/interpretable-ml-book/interaction.html#feature-interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SHQ6oh1uFzHO"
   },
   "outputs": [],
   "source": [
    "columns = ['Price', 'Good Location', 'Big Size']\n",
    "\n",
    "data = [[300000, 1, 1], \n",
    "        [200000, 1, 0], \n",
    "        [250000, 0, 1], \n",
    "        [150000, 0, 0]]\n",
    "\n",
    "house = pd.DataFrame(columns=columns, data=data)\n",
    "house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cH8IVe6GGAsm"
   },
   "source": [
    "#### Compare Linear Regression ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9uMiKdzF6Ud"
   },
   "outputs": [],
   "source": [
    "house_X = house.drop(columns='Price')\n",
    "house_y = house['Price']\n",
    "lr = LinearRegression()\n",
    "lr.fit(house_X, house_y)\n",
    "print('R^2', lr.score(house_X, house_y))\n",
    "print('Intercept \\t', lr.intercept_)\n",
    "coefficients = pd.Series(lr.coef_, house_X.columns)\n",
    "print(coefficients.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7AASlWVGMD4"
   },
   "source": [
    "#### ... versus a Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWlqO_usGKS8"
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(house_X, house_y)\n",
    "print('R^2', tree.score(house_X, house_y))\n",
    "viztree(tree, feature_names=house_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZkaOREUGzYp"
   },
   "source": [
    "### Simple housing, with a twist: _Feature Interaction_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HE0_pibGzJj"
   },
   "outputs": [],
   "source": [
    "house.loc[0, 'Price'] = 400000\n",
    "house_X = house.drop(columns='Price')\n",
    "house_y = house['Price']\n",
    "house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rh7ZJe7GHCSp"
   },
   "source": [
    "#### Compare Linear Regression ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YYINxJkdG_Q2"
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(house_X, house_y)\n",
    "print('R^2', lr.score(house_X, house_y))\n",
    "print('Intercept \\t', lr.intercept_)\n",
    "coefficients = pd.Series(lr.coef_, house_X.columns)\n",
    "print(coefficients.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9KIVsFO_HSmS"
   },
   "source": [
    "#### ... versus a Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6JYZBZBHIX2"
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(house_X, house_y)\n",
    "print('R^2', tree.score(house_X, house_y))\n",
    "viztree(tree, feature_names=house_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUW3dabWAWPk"
   },
   "source": [
    "## Get and interpret feature importances of a tree-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-1_grdR-AVyJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lesson_kaggle_challenge_1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
