{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# # If you're on Colab:\n",
    "# if 'google.colab' in sys.modules:\n",
    "#     DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge/master/data/'\n",
    "#     !pip install category_encoders==2.*\n",
    "\n",
    "# # If you're working locally:\n",
    "# else:\n",
    "DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge/master/data/'\n",
    "!pip install pandas-profiling==2.7.1\n",
    "!pip install category_encoders==2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merge train_features.csv & train_labels.csv\n",
    "train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'), \n",
    "                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n",
    "\n",
    "# Read test_features.csv & sample_submission.csv\n",
    "test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Wrangle the modeling data\n",
    "\n",
    "# indicate_missing is a function that returns a boolean value if the inbound data = 'MISSING'\n",
    "#   - helps in creating a \"missing\" column \n",
    "def indicate_missing(val):\n",
    "  if val == 'MISSING':\n",
    "    return True\n",
    "\n",
    "  return False\n",
    "\n",
    "# boolean_missing is a function converting the permit boolean column to categorical data \n",
    "def boolean_missing(val):\n",
    "  if val == True:\n",
    "    return 'TRUE'\n",
    "\n",
    "  if val == False:\n",
    "    return 'FALSE'\n",
    "\n",
    "  return 'MISSING'\n",
    "\n",
    "def wrangle(DF):\n",
    "  X = DF.copy()\n",
    "\n",
    "  # Replace near zero latitude values with zero\n",
    "  X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
    "\n",
    "  # Replace zero values with nan so we can impute values downstream\n",
    "  cols_with_zeroes = ['longitude',\n",
    "                      'latitude',\n",
    "                      'construction_year',\n",
    "                      'gps_height',\n",
    "                      'population']\n",
    "  for col in cols_with_zeroes:\n",
    "    X[col] = X[col].replace(0, np.nan)   # replace zeros with nans\n",
    "    X[col+'_MISSING'] = X[col].isnull()  # create a missing indicator column\n",
    "\n",
    "  # Create columns for month and year recorded data\n",
    "  X['date_recorded']  = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
    "  X['year_recorded']  = X['date_recorded'].dt.year\n",
    "  X['month_recorded'] = X['date_recorded'].dt.month\n",
    "\n",
    "  # Create a column reflecting the number of years from construction to year recorded\n",
    "  X['years']          = X['year_recorded'] - X['construction_year']\n",
    "  X['years']          = X['years'].replace(0, np.nan)   # replace zeros with nans\n",
    "  X['years_MISSING']  = X['years'].isnull() # create a missing years indicator column\n",
    "\n",
    "  # Replace missing boolean data with categorical data reflecting that missing data\n",
    "  cols_boolean_missing = ['public_meeting', 'permit']\n",
    "  for col in cols_boolean_missing:\n",
    "    X[col+'_CATEGORICAL']             = X[col].apply(boolean_missing)\n",
    "    X[col+'_CATEGORICAL'+'_MISSING']  = X[col+'_CATEGORICAL'].apply(indicate_missing)\n",
    "\n",
    "  # Replace missing categorical data with 'MISSING'\n",
    "  cols_categorical_missing = ['funder', 'installer', 'scheme_name', 'scheme_management', 'subvillage']\n",
    "  for col in cols_categorical_missing:\n",
    "    X[col]            = X[col].replace(np.nan, 'MISSING')   # replace zeros with nans\n",
    "    X[col+'_MISSING'] = X[col].apply(indicate_missing)\n",
    "\n",
    "  # List columns to be dropped\n",
    "  cols_drop = [\n",
    "               'date_recorded',             # date_recorded - using year_recorded and month_recorded instead\n",
    "               'quantity_group',            # duplicate column\n",
    "               'payment_type',              # duplicate column\n",
    "               'recorded_by',               # data collection process column (not predictive)\n",
    "               'id',                        # data collection process column (not predictive)\n",
    "               'permit',                    # replaced by categorical column: permit_string\n",
    "               'num_private',               # 98% zeroes, unclear the purpose of this dat\n",
    "               'construction_year',         # use 'years' as a proxy\n",
    "               'construction_year_MISSING', # use 'years_MISSING' as a proxy\n",
    "               'amount_tsh']                # highly skewed data\n",
    "  # Also drop the columns we processed due to missing values\n",
    "  cols_drop.extend(cols_boolean_missing)\n",
    "\n",
    "  # Drop undesired columns\n",
    "  X = X.drop(columns=cols_drop)\n",
    "\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (59400, 48) Test shape: (14358, 47)\n"
     ]
    }
   ],
   "source": [
    "# initial wrangle for train and test data\n",
    "df_train = wrangle(train)\n",
    "df_test  = wrangle(test)\n",
    "\n",
    "print(f'Train shape: {df_train.shape} Test shape: {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73087eb526545548b6da3c51cf9e3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Summarize dataset', max=55.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75933340e94f4ea0a7af41e0f932e962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Generate report structure', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce839fa6eff44b7c9f0e7a619d2a020d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Render HTML', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60928b6860e4f6ba368e39eb9f0b56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Export report to file', max=1.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8bb7bbce71423e87981c7fc16da393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Summarize dataset', max=54.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcc1e9cdb654653af4a313eae6bc777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Generate report structure', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b7f2fdf448493aa60321a976aa7516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Render HTML', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d43f7a728ea442b95f1a76b4758a86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Export report to file', max=1.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import pandas profiling package\n",
    "import pandas_profiling\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# Generate and download profile reports for the wrangled train and test datasets\n",
    "ProfileReport(df_train, minimal=True).to_file(output_file=\"tanzania_train_output.html\")\n",
    "ProfileReport(df_test,  minimal=True).to_file(output_file=\"tanzania_test_output.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The status_group column is the target\n",
    "target = 'status_group'\n",
    "\n",
    "# Define the training model inputs and target columns\n",
    "features = df_train.columns.drop(target)\n",
    "X_train  = df_train[features]\n",
    "y_train  = df_train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28445                 functional\n",
       "43507                 functional\n",
       "28560             non functional\n",
       "39597                 functional\n",
       "8833                  functional\n",
       "36335                 functional\n",
       "57510             non functional\n",
       "35478             non functional\n",
       "20763             non functional\n",
       "44373                 functional\n",
       "16722    functional needs repair\n",
       "31954                 functional\n",
       "14511    functional needs repair\n",
       "33117             non functional\n",
       "22328                 functional\n",
       "Name: status_group, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Construct the Cross Validation pipeline and fold configuration\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    RandomForestClassifier(random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Configure ranges of hyperparameter distributions\n",
    "param_dist = {\n",
    "    'randomforestclassifier__n_estimators': randint(50, 500),\n",
    "    'randomforestclassifier__max_depth': [5, 10, 15, 20, None],\n",
    "    'randomforestclassifier__max_features': uniform(0, 1)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring=None,\n",
    "    verbose=10,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58227        functional\n",
       "8621         functional\n",
       "54240    non functional\n",
       "40185    non functional\n",
       "51001    non functional\n",
       "26420        functional\n",
       "58139        functional\n",
       "51220    non functional\n",
       "5728         functional\n",
       "11243        functional\n",
       "37570        functional\n",
       "49809        functional\n",
       "7512         functional\n",
       "44042        functional\n",
       "50874        functional\n",
       "Name: status_group, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  30 | elapsed:  2.5min remaining:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  30 | elapsed:  3.5min remaining:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  30 | elapsed:  4.5min remaining:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed:  4.9min remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  30 | elapsed:  5.1min remaining:   33.7s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('ordinalencoder',\n",
       "                                              OrdinalEncoder(cols=None,\n",
       "                                                             drop_invariant=False,\n",
       "                                                             handle_missing='value',\n",
       "                                                             handle_unknown='value',\n",
       "                                                             mapping=None,\n",
       "                                                             return_df=True,\n",
       "                                                             verbose=0)),\n",
       "                                             ('simpleimputer',\n",
       "                                              SimpleImputer(add_indicator=False,\n",
       "                                                            copy=True,\n",
       "                                                            fill_value=None,\n",
       "                                                            missing_values=nan,\n",
       "                                                            strategy='mean',\n",
       "                                                            verbose=0)...\n",
       "                   param_distributions={'randomforestclassifier__max_depth': [5,\n",
       "                                                                              10,\n",
       "                                                                              15,\n",
       "                                                                              20,\n",
       "                                                                              None],\n",
       "                                        'randomforestclassifier__max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001CCE9E0F848>,\n",
       "                                        'randomforestclassifier__n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001CCE9FAB188>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=True, scoring=None, verbose=10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters {'randomforestclassifier__max_depth': 20, 'randomforestclassifier__max_features': 0.7946634753939757, 'randomforestclassifier__n_estimators': 369}\n",
      "Cross-validation MAE -0.8017845117845118\n"
     ]
    }
   ],
   "source": [
    "print('Best hyperparameters', search.best_params_)\n",
    "print('Cross-validation MAE', -search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "pipeline_best = search.best_estimator_\n",
    "\n",
    "X_test = df_test[features]\n",
    "\n",
    "# Generate model 'status_group' predictions for test dataset\n",
    "y_pred = pipeline_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
