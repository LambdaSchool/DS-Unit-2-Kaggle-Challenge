{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 2, Module 3*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "\n",
    "## Assignment\n",
    "- [ ] [Review requirements for your portfolio project](https://lambdaschool.github.io/ds/unit2), then submit your dataset.\n",
    "- [ ] Continue to participate in our Kaggle challenge. \n",
    "- [ ] Use scikit-learn for hyperparameter optimization with RandomizedSearchCV.\n",
    "- [ ] Submit your predictions to our Kaggle competition. (Go to our Kaggle InClass competition webpage. Use the blue **Submit Predictions** button to upload your CSV file. Or you can use the Kaggle API to submit your predictions.)\n",
    "- [ ] Commit your notebook to your fork of the GitHub repo.\n",
    "\n",
    "\n",
    "You won't be able to just copy from the lesson notebook to this assignment.\n",
    "\n",
    "- Because the lesson was ***regression***, but the assignment is ***classification.***\n",
    "- Because the lesson used [TargetEncoder](https://contrib.scikit-learn.org/categorical-encoding/targetencoder.html), which doesn't work as-is for _multi-class_ classification.\n",
    "\n",
    "So you will have to adapt the example, which is good real-world practice.\n",
    "\n",
    "1. Use a model for classification, such as [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "2. Use hyperparameters that match the classifier, such as `randomforestclassifier__ ...`\n",
    "3. Use a metric for classification, such as [`scoring='accuracy'`](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)\n",
    "4. If you’re doing a multi-class classification problem — such as whether a waterpump is functional, functional needs repair, or nonfunctional — then use a categorical encoding that works for multi-class classification, such as [OrdinalEncoder](https://contrib.scikit-learn.org/categorical-encoding/ordinal.html) (not [TargetEncoder](https://contrib.scikit-learn.org/categorical-encoding/targetencoder.html))\n",
    "\n",
    "\n",
    "\n",
    "## Stretch Goals\n",
    "\n",
    "### Reading\n",
    "- Jake VanderPlas, [Python Data Science Handbook, Chapter 5.3](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html), Hyperparameters and Model Validation\n",
    "- Jake VanderPlas, [Statistics for Hackers](https://speakerdeck.com/jakevdp/statistics-for-hackers?slide=107)\n",
    "- Ron Zacharski, [A Programmer's Guide to Data Mining, Chapter 5](http://guidetodatamining.com/chapter5/), 10-fold cross validation\n",
    "- Sebastian Raschka, [A Basic Pipeline and Grid Search Setup](https://github.com/rasbt/python-machine-learning-book/blob/master/code/bonus/svm_iris_pipeline_and_gridsearch.ipynb)\n",
    "- Peter Worcester, [A Comparison of Grid Search and Randomized Search Using Scikit Learn](https://blog.usejournal.com/a-comparison-of-grid-search-and-randomized-search-using-scikit-learn-29823179bc85)\n",
    "\n",
    "### Doing\n",
    "- Add your own stretch goals!\n",
    "- Try other [categorical encodings](https://contrib.scikit-learn.org/categorical-encoding/). See the previous assignment notebook for details.\n",
    "- In additon to `RandomizedSearchCV`, scikit-learn has [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Another library called scikit-optimize has [`BayesSearchCV`](https://scikit-optimize.github.io/notebooks/sklearn-gridsearchcv-replacement.html). Experiment with these alternatives.\n",
    "- _[Introduction to Machine Learning with Python](http://shop.oreilly.com/product/0636920030515.do)_ discusses options for \"Grid-Searching Which Model To Use\" in Chapter 6:\n",
    "\n",
    "> You can even go further in combining GridSearchCV and Pipeline: it is also possible to search over the actual steps being performed in the pipeline (say whether to use StandardScaler or MinMaxScaler). This leads to an even bigger search space and should be considered carefully. Trying all possible solutions is usually not a viable machine learning strategy. However, here is an example comparing a RandomForestClassifier and an SVC ...\n",
    "\n",
    "The example is shown in [the accompanying notebook](https://github.com/amueller/introduction_to_ml_with_python/blob/master/06-algorithm-chains-and-pipelines.ipynb), code cells 35-37. Could you apply this concept to your own pipelines?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Stacking!\n",
    "\n",
    "Here's some code you can use to \"stack\" multiple submissions, which is another form of ensembling:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Filenames of your submissions you want to ensemble\n",
    "files = ['submission-01.csv', 'submission-02.csv', 'submission-03.csv']\n",
    "\n",
    "target = 'status_group'\n",
    "submissions = (pd.read_csv(file)[[target]] for file in files)\n",
    "ensemble = pd.concat(submissions, axis='columns')\n",
    "majority_vote = ensemble.mode(axis='columns')[0]\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "submission = sample_submission.copy()\n",
    "submission[target] = majority_vote\n",
    "submission.to_csv('my-ultimate-ensemble-submission.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# If you're on Colab:\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge/master/data/'\n",
    "    !pip install category_encoders==2.*\n",
    "\n",
    "# If you're working locally:\n",
    "else:\n",
    "    DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merge train_features.csv & train_labels.csv\n",
    "train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'), \n",
    "                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n",
    "\n",
    "# Read test_features.csv & sample_submission.csv\n",
    "test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 41), (14358, 40))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrangle Functions.  We are just going to keep building off the last few assignments\n",
    "import numpy as np\n",
    "\n",
    "def wrangle(df):\n",
    "\n",
    "  #deep copy since we are changing values and the shape of our data. Don't want\n",
    "  #any warnings.\n",
    "  df = df.copy()\n",
    "\n",
    "  #Changing the almost zeros to zero on latitude.  Zero latitude is definitely\n",
    "  #a mistake, as it is on another part of the world\n",
    "\n",
    "  df['latitude'] = df['latitude'].replace(-2e08, 0)\n",
    "\n",
    "  #last assignment I explored and found what columns had too many zeros, so we\n",
    "  #are just going to build off that.\n",
    "  columns = ['gps_height', 'longitude', 'latitude', 'population', \n",
    "             'construction_year']\n",
    "\n",
    "  for column in columns:\n",
    "    df[column].replace(0, np.nan, inplace = True)\n",
    "\n",
    "    #I'm taking this from the lecture note book.  I suspect what we are doing\n",
    "    #is creating a boolean column, to identify which of our data is collected\n",
    "    #vs imputed(happening later in the pipeline).  Then our model may make\n",
    "    #better decisions, because it can weigh the importance of imputed vs\n",
    "    #collected data.  Just a theory.\n",
    "\n",
    "    #quote from Xander: missing values may be a predictive signal\n",
    "    df[column+'_MISSING'] = df[column].isnull()\n",
    "\n",
    "  #drop the dupes\n",
    "  df.drop(columns = ['quantity_group', 'payment_type'], inplace = True)\n",
    "\n",
    "  #drop never varying, and always varying columns\n",
    "  df.drop(columns = ['recorded_by', 'id'], inplace = True)\n",
    "\n",
    "  #Convert date_recorded to dattime\n",
    "  df['date_recorded'] = pd.to_datetime(df['date_recorded'], \n",
    "                                       infer_datetime_format = True)\n",
    "  \n",
    "  #making date more usable by extracting month, day and year\n",
    "  df['year_recorded'] = df['date_recorded'].dt.year\n",
    "  df['month_recorded'] = df['date_recorded'].dt.month\n",
    "  df['day_recorded'] = df['date_recorded'].dt.day\n",
    "\n",
    "  #dropping the datetime column\n",
    "  df.drop(columns = 'date_recorded', inplace = True)\n",
    "\n",
    "  #Cool feature I'm borrowing from class: How many years from construction to\n",
    "  #date recorded\n",
    "  df['years'] = df['year_recorded'] - df['construction_year']\n",
    "  df['years_MISSING'] = df['years'].isnull()\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = wrangle(train)\n",
    "test = wrangle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as last assignment\n",
    "target = 'status_group'\n",
    "\n",
    "X_train = train.drop(columns = target)\n",
    "y_train = train[target]\n",
    "\n",
    "X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 45), (14358, 45))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay lets just set up the pipeline here\n",
    "import category_encoders as ce\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(),\n",
    "    RandomForestClassifier(n_jobs = -1, random_state = 42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okay lets go ahead and set up our randomized search CV.  Been really wanting to\n",
    "#give this a try.  My goal is to run it once, with some pretty big steps in my\n",
    "#ranges here.  Then if I want, maybe run it again with some smaller\n",
    "\n",
    "#Justification for my ranges.  Research indicates that minsamples split is most\n",
    "#reponsive in a range of 2-40, and min samples leafe is most responsive in a\n",
    "#range of 1-20.  Max leaf nodes I'm selecting off of past good result ranges.\n",
    "\n",
    "#I'm also using these ranges, because I'm not doing a wild amount of iterations.\n",
    "#if I were to do a crazy amount of iterations, then I'd be better served by using\n",
    "#randint over a range, to get better granularity.\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "param_distributions = {\n",
    "    'simpleimputer__strategy': ['mean', 'median'],\n",
    "    'randomforestclassifier__max_leaf_nodes': randint(1, 10000),\n",
    "    'randomforestclassifier__min_samples_split': randint(2, 40),\n",
    "    'randomforestclassifier__min_samples_leaf': randint(1, 20),\n",
    "    'randomforestclassifier__max_depth': randint(1, 20),\n",
    "    'randomforestclassifier__max_features': uniform(0, 1)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions = param_distributions,\n",
    "    n_iter = 100,\n",
    "    cv = 5,\n",
    "    scoring = 'accuracy',\n",
    "    verbose = 10,\n",
    "    return_train_score = True,\n",
    "    n_jobs = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   41.7s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   49.7s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   59.4s\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 289 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 314 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 397 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done 457 tasks      | elapsed: 15.3min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 16.5min finished\n"
     ]
    }
   ],
   "source": [
    "#okay lets give it a go\n",
    "best_model = search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'randomforestclassifier__max_depth': 18, 'randomforestclassifier__max_features': 0.6691986778653586, 'randomforestclassifier__max_leaf_nodes': 2804, 'randomforestclassifier__min_samples_leaf': 3, 'randomforestclassifier__min_samples_split': 5, 'simpleimputer__strategy': 'median'}\n",
      "Cross-Validation Score: 0.8074242424242424\n"
     ]
    }
   ],
   "source": [
    "print('Best hyperparameters:', search.best_params_)\n",
    "print('Cross-Validation Score:', search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm definitely not excited about that score.  But it is an mvp, so I need to complete MVP\n",
    "#first, and come back to play later.\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "submission = sample_submission.copy()\n",
    "submission['status_group'] = y_pred\n",
    "submission.to_csv('rand_search_cross_val_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
