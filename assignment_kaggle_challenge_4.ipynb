{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science, Unit 2: Predictive Modeling\n",
    "\n",
    "# Kaggle Challenge, Module 4\n",
    "\n",
    "## Catch up, if needed\n",
    "- [Review requirements for your portfolio project](https://lambdaschool.github.io/ds/unit2/portfolio-project/ds6), then choose your dataset, and [submit this form](https://forms.gle/nyWURUg65x1UTRNV9), due yesterday at 3:59pm Pacific.\n",
    "- Submit predictions to our Kaggle competition. (Go to our Kaggle InClass competition webpage. Use the blue **Submit Predictions** button to upload your CSV file.) The competition closes today at 3:59pm. Every student should make at least one submission that scores at least 60% accuracy (above the majority class baseline).\n",
    "\n",
    "## Assignment\n",
    "- [X] Continue to participate in our Kaggle challenge. \n",
    "- [X] Use scikit-learn for hyperparameter optimization with RandomizedSearchCV.\n",
    "- [X] Submit your final predictions to our Kaggle competition. Optionally, go to **My Submissions**, and _\"you may select up to 1 submission to be used to count towards your final leaderboard score.\"_ The competition closes today at 3:59pm.\n",
    "- [ ] Add comments and Markdown to your notebook. Clean up your code.\n",
    "- [ ] Commit your notebook to your fork of the GitHub repo.\n",
    "\n",
    "## Stretch Goals\n",
    "\n",
    "### Reading\n",
    "- [ ] Jake VanderPlas, [Python Data Science Handbook, Chapter 5.3](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html), Hyperparameters and Model Validation\n",
    "- [ ] Jake VanderPlas, [Statistics for Hackers](https://speakerdeck.com/jakevdp/statistics-for-hackers?slide=107)\n",
    "- [ ] Ron Zacharski, [A Programmer's Guide to Data Mining, Chapter 5](http://guidetodatamining.com/chapter5/), 10-fold cross validation\n",
    "- [ ] Sebastian Raschka, [A Basic Pipeline and Grid Search Setup](https://github.com/rasbt/python-machine-learning-book/blob/master/code/bonus/svm_iris_pipeline_and_gridsearch.ipynb)\n",
    "- [ ] Peter Worcester, [A Comparison of Grid Search and Randomized Search Using Scikit Learn](https://blog.usejournal.com/a-comparison-of-grid-search-and-randomized-search-using-scikit-learn-29823179bc85)\n",
    "\n",
    "### Doing\n",
    "- Try combining xgboost early stopping, cross-validation, & hyperparameter optimization, with [the \"original\" (non scikit-learn) xgboost API](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.cv).\n",
    "- In additon to `RandomizedSearchCV`, scikit-learn has [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Another library called scikit-optimize has [`BayesSearchCV`](https://scikit-optimize.github.io/notebooks/sklearn-gridsearchcv-replacement.html). Experiment with these alternatives.\n",
    "- _[Introduction to Machine Learning with Python](http://shop.oreilly.com/product/0636920030515.do)_ discusses options for \"Grid-Searching Which Model To Use\" in Chapter 6:\n",
    "\n",
    "> You can even go further in combining GridSearchCV and Pipeline: it is also possible to search over the actual steps being performed in the pipeline (say whether to use StandardScaler or MinMaxScaler). This leads to an even bigger search space and should be considered carefully. Trying all possible solutions is usually not a viable machine learning strategy. However, here is an example comparing a RandomForestClassifier and an SVC ...\n",
    "\n",
    "The example is shown in [the accompanying notebook](https://github.com/amueller/introduction_to_ml_with_python/blob/master/06-algorithm-chains-and-pipelines.ipynb), code cells 35-37. Could you apply this concept to your own pipelines?\n",
    "\n",
    "#### Try stacking multiple submissions!\n",
    "\n",
    "Here's some code you can use:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Filenames of your submissions you want to ensemble\n",
    "files = ['submission-01.csv', 'submission-02.csv', 'submission-03.csv']\n",
    "\n",
    "target = 'status_group'\n",
    "submissions = (pd.read_csv(file)[[target]] for file in files)\n",
    "ensemble = pd.concat(submissions, axis='columns')\n",
    "majority_vote = ensemble.mode(axis='columns')[0]\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "submission = sample_submission.copy()\n",
    "submission[target] = majority_vote\n",
    "submission.to_csv('my-ultimate-ensemble-submission.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the imports out of the way.\n",
    "# Don't worry...we'll use them all eventually...\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import category_encoders as ce\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "\n",
    "# Load the training and testing data\n",
    "train = pd.merge(pd.read_csv('train_features.csv'),\n",
    "                 pd.read_csv('train_labels.csv'))\n",
    "test = pd.read_csv('test_features.csv')\n",
    "\n",
    "# Load the sample submission so we can use it at the end.\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Split the training set so we can get our validation set\n",
    "train, val = train_test_split(train, train_size=0.50, test_size=0.50,\n",
    "                             stratify=train['status_group'], random_state=42)\n",
    "\n",
    "# Get X_train and y_train ready \n",
    "target = 'status_group'\n",
    "X_train = train.drop(columns=target)\n",
    "y_train = train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# Making a pipeline\n",
    "pipeline = make_pipeline(\n",
    "    ce.OneHotEncoder(use_cat_names=True),\n",
    "    SimpleImputer(),\n",
    "    SelectKBest(f_regression),\n",
    "    Ridge()\n",
    ")\n",
    "\n",
    "# The hyperparameters we'll be looking at\n",
    "param_dists = {\n",
    "    'simpleimputer__strategy': ['mean', 'median'],\n",
    "    'selectkbest__k': randint(1, len(X_train.columns)+1),\n",
    "    'ridge__alpha': uniform(1,10)\n",
    "}\n",
    "\n",
    "# Getting the search ready\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions = param_dists,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    verbose=10,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1 # Number of cpu cores\n",
    ")\n",
    "\n",
    "# Search for the best\n",
    "search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best hyperparameters', search.best_params_)\n",
    "print('Cross-validation MAE', -search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:   18.2s remaining:   42.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   21.9s remaining:   21.9s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:   45.0s remaining:   19.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2, error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('ordinalencoder',\n",
       "                                              OrdinalEncoder(cols=None,\n",
       "                                                             drop_invariant=False,\n",
       "                                                             handle_missing='value',\n",
       "                                                             handle_unknown='value',\n",
       "                                                             mapping=None,\n",
       "                                                             return_df=True,\n",
       "                                                             verbose=0)),\n",
       "                                             ('simpleimputer',\n",
       "                                              SimpleImputer(add_indicator=False,\n",
       "                                                            copy=True,\n",
       "                                                            fill_value=None,\n",
       "                                                            missing_values=nan,\n",
       "                                                            strategy='...\n",
       "                   param_distributions={'xgbregressor__learning_rate': [0.0001,\n",
       "                                                                        0.001,\n",
       "                                                                        0.01,\n",
       "                                                                        0.1,\n",
       "                                                                        0.2,\n",
       "                                                                        0.3],\n",
       "                                        'xgbregressor__max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000026B890294E0>,\n",
       "                                        'xgbregressor__n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000026B88F46278>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=True, scoring='neg_mean_absolute_error',\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning, module='xgboost')\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(),\n",
    "    XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    ")\n",
    "\n",
    "param_distributions = {\n",
    "    'xgbregressor__n_estimators': randint(100,1000),\n",
    "    'xgbregressor__max_depth': randint(3,10),\n",
    "    'xgbregressor__learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=5,\n",
    "    cv=2,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    verbose=10,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters {'xgbregressor__learning_rate': 0.2, 'xgbregressor__max_depth': 4, 'xgbregressor__n_estimators': 994}\n",
      "Cross-validation MAE 420.16267880853167\n"
     ]
    }
   ],
   "source": [
    "print('Best hyperparameters', search.best_params_)\n",
    "print('Cross-validation MAE', -search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
