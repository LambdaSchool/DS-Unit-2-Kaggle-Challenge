{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bit0d141f66d7614575996f4a537ba1c30a",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint Challenge 22 Review"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Begin with baselines for classification.* What is your baseline accuracy, if you guessed the majority class for every prediction?\n",
    "\n",
    "2. Hold out your test set. (Time-based)*\n",
    "\n",
    "3. Engineer new feature.* Engineer at least 1 new feature, from a provided list, or your own idea.\n",
    "\n",
    "4. Decide how to validate your model.* Choose one of the following options. Any of these options are good. You are not graded on which you choose.\n",
    "   - *Train/validate/test split: time-based*\n",
    "   - *Train/validate/test split: random 80/20%* train/validate split.\n",
    "   - *Cross-validation* with independent test set. You may use any scikit-learn cross-validation method.\n",
    "\n",
    "5. Use a scikit-learn *pipeline* to *encode categoricals* and fit a *Decision Tree* or *Random Forest* model.\n",
    "\n",
    "6. Get your model’s *validation accuracy.* (Multiple times if you try multiple iterations.)\n",
    "\n",
    "7. Get your model’s *test accuracy.* (One time, at the end.)\n",
    "\n",
    "8. Given a *confusion matrix* for a hypothetical binary classification model, *calculate accuracy, precision, and recall.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 221 - Decision Trees"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points:\n",
    "* Handling Outliers\n",
    "* Pipelines\n",
    "* Basic Decision Tree Classifer\n",
    "* If none of the leaf nodes contain 100% of a certain group, they're considered 'impure'\n",
    "\n",
    "### Types of Impurities:\n",
    "Gini impurity: \n",
    "* 1 - (probability of 'yes')^2 - (probability of 'no')^2\n",
    "* total Gini impurity is the weighted average of each lead node Impurities\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers\n",
    "\n",
    "Common issues:\n",
    "* zeroes listed in-place of missing values (NaNs)\n",
    "* values that should be zero instead listed as very close to zero (2.8e-10)\n",
    "\n",
    "Pandas Profiling:\n",
    "Open source Python module with which we can quickly do an exploratory data analysis with just a few lines of code. Also generates interactive reports in web format that can be presented to any person, even if they don't know programming.\n",
    "\n",
    "What does Pandas Profiling show?\n",
    "* % of values that are unique\n",
    "* % of values that are NaN \n",
    "* data is skewed\n",
    "* data contains high cardinality values \n",
    "* "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "profile = ProfileReport(train, minimal=True).to_notebook_iframe()\n",
    "\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "What is a Pipeline?\n",
    "* A pipeline is a tool that combines the processes of multiple processes into a single process.\n",
    "* You only have to call fit and predict once to fit a whole sequence of estimators\n",
    "\n",
    "Processes that pipeline replaces:\n",
    "* Encode (OneHotEncoder)\n",
    "* Imputer (ScaledImputer())\n",
    "* Scaler (StandardScaler())\n",
    "* Model (fit)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Decision Tree Classifier"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree compontents\n",
    "* Nodes\n",
    "    - Test for the value of a certain attribute\n",
    "* Edges/Branch\n",
    "  - Correspond to the outcome of a test and connect to the next node or leaf\n",
    "* Leaf Nodes\n",
    "  - Terminal nodes that predict the outcome (represent class labels or class distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Decision Trees\n",
    "* Classification Trees (categorical variables)\n",
    "* Regression Trees (continuous data types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier\n",
    "* Using the decision algorithm, we start at the tree root and split the data on the feature that results in the largest ***information gain (IG)*** (reduction in uncertainty towards the final decision).\n",
    "* In an iterative process, we can then repeat this splitting procedure at each child node ***until the leaves are pure***. This means that the samples at each leaf node all belong to the same class.\n",
    "* In practice, we may set a ***limit on the depth of the tree to prevent overfitting***. We compromise on purity here somewhat as the final leaves may still have some impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of Classification with Decision Trees\n",
    "* Inexpensive to contruct\n",
    "* Extermely fast at classifying unknown records\n",
    "* Easy to interpret for small-sized trees \n",
    "* Accuracy comparable to other classification techniques for many simple data sets\n",
    "* Excludes unimportant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantages of Classification with Decision Trees\n",
    "* Easy to overfit\n",
    "* Decision Voundary restricted to being parallel to attribute axes\n",
    "* Decision tree models are often biased toward splits on features having a large number of levels\n",
    "* Small changes in the training data can result in large changes to decision logic\n",
    "* Large trees can be difficult to interpret and the decision they make may seem counter intuitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applications of Decision Trees in real life\n",
    "* Biomedical Engineering (decision trees for indentifying features to be used in implantable devices)\n",
    "* Financial analysis (Customer Satisfaction with a product or service)\n",
    "* Astronomy (classify galaxies)\n",
    "* System Control\n",
    "* Manufacturing and Production (Quality control, Semiconductor Manufacturing, etc)\n",
    "* Medicines (diagnosis, cardiology, psychiatry)\n",
    "* Physics (particle detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complexity vs. Consistency\n",
    "One 'split' decision tree is the most consistently wrong, but not very complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 222 - Random Forests\n",
    "\n",
    "* ordinal encoding with random forests \n",
    "* ordinal encoding is more flexible\n",
    "\n",
    "### How does it work? Explain to a recruiter\n",
    "\n",
    "### Quiz notes\n",
    "* LOGISTIC REGRESSION IS A CLASSIFICATION TECHNIQUE\n",
    "* number of estimators and maximum tree depth increase chance of overfitting\n",
    "* difference in criterion before the split (parent) to criterion after this split (child)\n",
    "* Which of the following are evaluation metrics we might consider for a logistic regression? ROC/AUC (area under the curve), Precision, True Positive Rate, Sensitivity, Specificity\n",
    "* Key Points for describing logistic regression and sigmoid\n",
    "   * Between 0 and 1\n",
    "   * The amount a number has to round determines it's confidence of yes or no \n",
    "   * [Paper on different types of prediction](arxiv.org/pdf/1708.05070.pdf)\n",
    "   * Look for additional article link in Slack from 4/15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 223 - Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When do you use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 224 - Classification Metrics\n",
    "* get and interpret the confusion matrix for classification models\n",
    "* use classification metrics: precision, recall\n",
    "understand the relationships between precision, recall, thresholds, and predicted probabilities, to * help make decisions and allocate budgets\n",
    "* Get ROC AUC (Receiver Operating Characteristic, Area Under the Curve) (Keri's favorite - used commonly in the medical field)\n",
    "* false negative rate - type 2 error\n",
    "* false positive rate - type 1 error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report: Precision and Recall\n",
    "\n",
    "[Scikit-Learn User Guide — Classification Report](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report)\n",
    "\n",
    "You will see something called F1 score, alongside precision and recall, but Keri said it's not useful. Accuracy describes precision, recall and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision:\n",
    "\n",
    "* true_positives / (true_positives + false_positives)\n",
    "* How many selected items are relevant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall: also Sensitivity (more statistics focused)\n",
    "\n",
    "* true_positive / (true_positive + false_negative)\n",
    "* How many relevant items are selected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC (Receiver Operating Characteristic, Area Under the Curve)\n",
    "\n",
    "* higher area under the curve is better, want to aim for a 'perfect square' to maximize area\n",
    "* random guess, AUC of 0.5 (y=x is the 'curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Score\n",
    "\n",
    "* percentage of the way to 1.0 aka perfect square?"
   ]
  }
 ]
}