{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bit0d141f66d7614575996f4a537ba1c30a",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint Challenge 22 Review"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "* Handling Outliers\n",
    "* Pipelines\n",
    "* Basic Decision Tree Classifer"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers\n",
    "\n",
    "Common issues:\n",
    "* zeroes listed in-place of missing values (NaNs)\n",
    "* values that should be zero instead listed as very close to zero (2.8e-10)\n",
    "\n",
    "Pandas Profiling:\n",
    "Open source Python module with which we can quickly do an exploratory data analysis with just a few lines of code. Also generates interactive reports in web format that can be presented to any person, even if they don't know programming.\n",
    "\n",
    "What does Pandas Profiling show?\n",
    "* % of values that are unique\n",
    "* % of values that are NaN \n",
    "* data is skewed\n",
    "* data contains high cardinality values \n",
    "* "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "profile = ProfileReport(train, minimal=True).to_notebook_iframe()\n",
    "\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "What is a Pipeline?\n",
    "* A pipeline is a tool that combines the processes of multiple processes into a single process.\n",
    "* You only have to call fit and predict once to fit a whole sequence of estimators\n",
    "\n",
    "Processes that pipeline replaces:\n",
    "* Encode (OneHotEncoder)\n",
    "* Imputer (ScaledImputer())\n",
    "* Scaler (StandardScaler())\n",
    "* Model (fit)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Decision Tree Classifier"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree compontents\n",
    "* Nodes\n",
    "    - Test for the value of a certain attribute\n",
    "* Edges/Branch\n",
    "  - Correspond to the outcome of a test and connect to the next node or leaf\n",
    "* Leaf Nodes\n",
    "  - Terminal nodes that predict the outcome (represent class labels or class distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Decision Trees\n",
    "* Classification Trees (categorical variables)\n",
    "* Regression Trees (continuous data types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier\n",
    "* Using the decision algorithm, we start at the tree root and split the data on the feature that results in the largest ***information gain (IG)*** (reduction in uncertainty towards the final decision).\n",
    "* In an iterative process, we can then repeat this splitting procedure at each child node ***until the leaves are pure***. This means that the samples at each leaf node all belong to the same class.\n",
    "* In practice, we may set a ***limit on the depth of the tree to prevent overfitting***. We compromise on purity here somewhat as the final leaves may still have some impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of Classification with Decision Trees\n",
    "* Inexpensive to contruct\n",
    "* Extermely fast at classifying unknown records\n",
    "* Easy to interpret for small-sized trees \n",
    "* Accuracy comparable to other classification techniques for many simple data sets\n",
    "* Excludes unimportant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantages of Classification with Decision Trees\n",
    "* Easy to overfit\n",
    "* Decision Voundary restricted to being parallel to attribute axes\n",
    "* Decision tree models are often biased toward splits on features having a large number of levels\n",
    "* Small changes in the training data can result in large changes to decision logic\n",
    "* Large trees can be difficult to interpret and the decision they make may seem counter intuitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applications of Decision Trees in real life\n",
    "* Biomedical Engineering (decision trees for indentifying features to be used in implantable devices)\n",
    "* Financial analysis (Customer Satisfaction with a product or service)\n",
    "* Astronomy (classify galaxies)\n",
    "* System Control\n",
    "* Manufacturing and Production (Quality control, Semiconductor Manufacturing, etc)\n",
    "* Medicines (diagnosis, cardiology, psychiatry)\n",
    "* Physics (particle detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complexity vs. Consistency\n",
    "One 'split' decision tree is the most consistently wrong, but not very complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ]
}