{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 2, Module 2*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7IXUfiQ2UKj6"
   },
   "source": [
    "# Random Forests\n",
    "\n",
    "## Assignment\n",
    "- [ ] Read [“Adopting a Hypothesis-Driven Workflow”](https://outline.com/5S5tsB), a blog post by a Lambda DS student about the Tanzania Waterpumps challenge.\n",
    "- [ ] Continue to participate in our Kaggle challenge.\n",
    "- [ ] Define a function to wrangle train, validate, and test sets in the same way. Clean outliers and engineer features.\n",
    "- [ ] Try Ordinal Encoding.\n",
    "- [ ] Try a Random Forest Classifier.\n",
    "- [ ] Submit your predictions to our Kaggle competition. (Go to our Kaggle InClass competition webpage. Use the blue **Submit Predictions** button to upload your CSV file. Or you can use the Kaggle API to submit your predictions.)\n",
    "- [ ] Commit your notebook to your fork of the GitHub repo.\n",
    "\n",
    "## Stretch Goals\n",
    "\n",
    "### Doing\n",
    "- [ ] Add your own stretch goal(s) !\n",
    "- [ ] Do more exploratory data analysis, data cleaning, feature engineering, and feature selection.\n",
    "- [ ] Try other [categorical encodings](https://contrib.scikit-learn.org/categorical-encoding/).\n",
    "- [ ] Get and plot your feature importances.\n",
    "- [ ] Make visualizations and share on Slack.\n",
    "\n",
    "### Reading\n",
    "\n",
    "Top recommendations in _**bold italic:**_\n",
    "\n",
    "#### Decision Trees\n",
    "- A Visual Introduction to Machine Learning, [Part 1: A Decision Tree](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/),  and _**[Part 2: Bias and Variance](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)**_\n",
    "- [Decision Trees: Advantages & Disadvantages](https://christophm.github.io/interpretable-ml-book/tree.html#advantages-2)\n",
    "- [How a Russian mathematician constructed a decision tree — by hand — to solve a medical problem](http://fastml.com/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/)\n",
    "- [How decision trees work](https://brohrer.github.io/how_decision_trees_work.html)\n",
    "- [Let’s Write a Decision Tree Classifier from Scratch](https://www.youtube.com/watch?v=LDRbO9a6XPU)\n",
    "\n",
    "#### Random Forests\n",
    "- [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/), Chapter 8: Tree-Based Methods\n",
    "- [Coloring with Random Forests](http://structuringtheunstructured.blogspot.com/2017/11/coloring-with-random-forests.html)\n",
    "- _**[Random Forests for Complete Beginners: The definitive guide to Random Forests and Decision Trees](https://victorzhou.com/blog/intro-to-random-forests/)**_\n",
    "\n",
    "#### Categorical encoding for trees\n",
    "- [Are categorical variables getting lost in your random forests?](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/)\n",
    "- [Beyond One-Hot: An Exploration of Categorical Variables](http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/)\n",
    "- _**[Categorical Features and Encoding in Decision Trees](https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931)**_\n",
    "- _**[Coursera — How to Win a Data Science Competition: Learn from Top Kagglers — Concept of mean encoding](https://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv)**_\n",
    "- [Mean (likelihood) encodings: a comprehensive study](https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study)\n",
    "- [The Mechanics of Machine Learning, Chapter 6: Categorically Speaking](https://mlbook.explained.ai/catvars.html)\n",
    "\n",
    "#### Imposter Syndrome\n",
    "- [Effort Shock and Reward Shock (How The Karate Kid Ruined The Modern World)](http://www.tempobook.com/2014/07/09/effort-shock-and-reward-shock/)\n",
    "- [How to manage impostor syndrome in data science](https://towardsdatascience.com/how-to-manage-impostor-syndrome-in-data-science-ad814809f068)\n",
    "- [\"I am not a real data scientist\"](https://brohrer.github.io/imposter_syndrome.html)\n",
    "- _**[Imposter Syndrome in Data Science](https://caitlinhudon.com/2018/01/19/imposter-syndrome-in-data-science/)**_\n",
    "\n",
    "\n",
    "### More Categorical Encodings\n",
    "\n",
    "**1.** The article **[Categorical Features and Encoding in Decision Trees](https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931)** mentions 4 encodings:\n",
    "\n",
    "- **\"Categorical Encoding\":** This means using the raw categorical values as-is, not encoded. Scikit-learn doesn't support this, but some tree algorithm implementations do. For example, [Catboost](https://catboost.ai/), or R's [rpart](https://cran.r-project.org/web/packages/rpart/index.html) package.\n",
    "- **Numeric Encoding:** Synonymous with Label Encoding, or \"Ordinal\" Encoding with random order. We can use [category_encoders.OrdinalEncoder](https://contrib.scikit-learn.org/categorical-encoding/ordinal.html).\n",
    "- **One-Hot Encoding:** We can use [category_encoders.OneHotEncoder](http://contrib.scikit-learn.org/categorical-encoding/onehot.html).\n",
    "- **Binary Encoding:** We can use [category_encoders.BinaryEncoder](http://contrib.scikit-learn.org/categorical-encoding/binary.html).\n",
    "\n",
    "\n",
    "**2.** The short video \n",
    "**[Coursera — How to Win a Data Science Competition: Learn from Top Kagglers — Concept of mean encoding](https://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv)** introduces an interesting idea: use both X _and_ y to encode categoricals.\n",
    "\n",
    "Category Encoders has multiple implementations of this general concept:\n",
    "\n",
    "- [CatBoost Encoder](http://contrib.scikit-learn.org/categorical-encoding/catboost.html)\n",
    "- [James-Stein Encoder](http://contrib.scikit-learn.org/categorical-encoding/jamesstein.html)\n",
    "- [Leave One Out](http://contrib.scikit-learn.org/categorical-encoding/leaveoneout.html)\n",
    "- [M-estimate](http://contrib.scikit-learn.org/categorical-encoding/mestimate.html)\n",
    "- [Target Encoder](http://contrib.scikit-learn.org/categorical-encoding/targetencoder.html)\n",
    "- [Weight of Evidence](http://contrib.scikit-learn.org/categorical-encoding/woe.html)\n",
    "\n",
    "Category Encoder's mean encoding implementations work for regression problems or binary classification problems. \n",
    "\n",
    "For multi-class classification problems, you will need to temporarily reformulate it as binary classification. For example:\n",
    "\n",
    "```python\n",
    "encoder = ce.TargetEncoder(min_samples_leaf=..., smoothing=...) # Both parameters > 1 to avoid overfitting\n",
    "X_train_encoded = encoder.fit_transform(X_train, y_train=='functional')\n",
    "X_val_encoded = encoder.transform(X_train, y_val=='functional')\n",
    "```\n",
    "\n",
    "For this reason, mean encoding won't work well within pipelines for multi-class classification problems.\n",
    "\n",
    "**3.** The **[dirty_cat](https://dirty-cat.github.io/stable/)** library has a Target Encoder implementation that works with multi-class classification.\n",
    "\n",
    "```python\n",
    " dirty_cat.TargetEncoder(clf_type='multiclass-clf')\n",
    "```\n",
    "It also implements an interesting idea called [\"Similarity Encoder\" for dirty categories](https://www.slideshare.net/GaelVaroquaux/machine-learning-on-non-curated-data-154905090).\n",
    "\n",
    "However, it seems like dirty_cat doesn't handle missing values or unknown categories as well as category_encoders does. And you may need to use it with one column at a time, instead of with your whole dataframe.\n",
    "\n",
    "**4. [Embeddings](https://www.kaggle.com/learn/embeddings)** can work well with sparse / high cardinality categoricals.\n",
    "\n",
    "_**I hope it’s not too frustrating or confusing that there’s not one “canonical” way to encode categoricals. It’s an active area of research and experimentation! Maybe you can make your own contributions!**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab (run the code cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9eSnDYhUGD7"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# If you're on Colab:\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge/master/data/'\n",
    "    !pip install category_encoders==2.*\n",
    "\n",
    "# If you're working locally:\n",
    "else:\n",
    "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge/master/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QJBD4ruICm1m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 41), (14358, 40))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'), \n",
    "                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n",
    "test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into train & val\n",
    "train, validate = train_test_split(\n",
    "    train, \n",
    "    train_size=0.80, \n",
    "    test_size=0.20, \n",
    "    stratify=train['status_group'], \n",
    "    random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(X):\n",
    "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
    "    \n",
    "    # Prevent SettingWithCopyWarning\n",
    "    X = X.copy()\n",
    "    \n",
    "    # About 3% of the time, latitude has small values near zero,\n",
    "    # outside Tanzania, so we'll treat these values like zero.\n",
    "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
    "    \n",
    "    # When columns have zeros and shouldn't, they are like null values.\n",
    "    # So we will replace the zeros with nulls, and impute missing values later.\n",
    "    # Also create a \"missing indicator\" column, because the fact that\n",
    "    # values are missing may be a predictive signal.\n",
    "    cols_with_zeros = ['longitude', 'latitude', 'construction_year', \n",
    "                       'gps_height', 'population']\n",
    "    for col in cols_with_zeros:\n",
    "        X[col] = X[col].replace(0, np.nan)\n",
    "        X[col+'_MISSING'] = X[col].isnull()\n",
    "            \n",
    "    # Drop duplicate columns\n",
    "    duplicates = ['quantity_group', 'payment_type']\n",
    "    X = X.drop(columns=duplicates)\n",
    "    \n",
    "    # Drop recorded_by (never varies) and id (always varies, random)\n",
    "    unusable_variance = ['recorded_by', 'id']\n",
    "    X = X.drop(columns=unusable_variance)\n",
    "    \n",
    "    # Convert date_recorded to datetime\n",
    "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
    "    \n",
    "    # Extract components from date_recorded, then drop the original column\n",
    "    X['year_recorded'] = X['date_recorded'].dt.year\n",
    "    X['month_recorded'] = X['date_recorded'].dt.month\n",
    "    X['day_recorded'] = X['date_recorded'].dt.day\n",
    "    X = X.drop(columns='date_recorded')\n",
    "    \n",
    "    # Engineer feature: how many years from construction_year to date_recorded\n",
    "    X['years'] = X['year_recorded'] - X['construction_year']\n",
    "    X['years_MISSING'] = X['years'].isnull()\n",
    "    \n",
    "    \n",
    "    # return the wrangled dataframe\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = wrangle(train)\n",
    "validate = wrangle(validate)\n",
    "test = wrangle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The status_group column is the target\n",
    "target = 'status_group'\n",
    "\n",
    "# Get a dataframe with all train columns except the target\n",
    "train_features = train.drop(columns=[target])\n",
    "\n",
    "# Get a list of the numeric features\n",
    "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# Get a series with the cardinality of the nonnumeric features\n",
    "cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
    "\n",
    "# Get a list of all categorical features with cardinality <= 50\n",
    "categorical_features = cardinality[cardinality<=50].index.tolist()\n",
    "\n",
    "# Combine the lists \n",
    "features = numeric_features + categorical_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange data into X features matrix and y target vector \n",
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "X_validate = validate[features]\n",
    "y_validate = validate[target]\n",
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9584385521885522\n",
      "Validation Accuracy 0.8153198653198653\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(strategy='median'),\n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier(bootstrap=True, max_depth=50, max_features='auto', min_samples_leaf= 1, min_samples_split= 5, n_estimators=800, random_state=42, criterion='entropy')\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print ('Train Accuracy', pipeline.score(X_train, y_train))\n",
    "print ('Validation Accuracy', pipeline.score(X_validate, y_validate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = ce.OrdinalEncoder().fit_transform(X_train)\n",
    "X_train_imputed = SimpleImputer().fit_transform(X_train_encoded)\n",
    "#X_train_scaled = StandardScaler().fit_transform(X_train_imputed)\n",
    "\n",
    "X_validate_encoded = ce.OrdinalEncoder().fit_transform(X_validate)\n",
    "X_validate_imputed = SimpleImputer().fit_transform(X_validate_encoded)\n",
    "#X_validate_scaled = StandardScaler().fit_transform(X_validate_imputed)\n",
    "\n",
    "X_test_encoded = ce.OrdinalEncoder().fit_transform(X_test)\n",
    "X_test_imputed = SimpleImputer().fit_transform(X_test_encoded)\n",
    "#X_test_scaled = StandardScaler().fit_transform(X_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': range(10, 1000, 200), 'criterion': ['gini', 'entropy'], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [2, 4, 8, 16, 32, 64], 'min_samples_split': [2, 4, 6, 8, 10], 'min_samples_leaf': [2, 4, 6, 8, 10], 'random_state': range(1, 42), 'bootstrap': [True, False]}\n",
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "/home/alex/anaconda3/envs/geoprocessing/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-2)]: Done  27 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-2)]: Done 148 tasks      | elapsed: 17.2min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-120272edb4c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mrf_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Fit the random search model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mrf_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_imputed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoprocessing/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoprocessing/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/geoprocessing/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoprocessing/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoprocessing/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoprocessing/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoprocessing/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoprocessing/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_estimators = range(10, 1000, 200)\n",
    "criterion = ['gini', 'entropy']\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = [2, 4, 8, 16, 32, 64]\n",
    "min_samples_split = [2, 4, 6, 8, 10]\n",
    "min_samples_leaf = [2, 4, 6, 8, 10]\n",
    "random_state = range(1, 42, 1)\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'criterion' : criterion,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'random_state' : random_state,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 1000, cv = 3, verbose=2, random_state=42, n_jobs=-2)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train_imputed, y_train)\n",
    "\n",
    "\n",
    "print ('Train Accuracy', pipeline.score(X_train_imputed, y_train))\n",
    "print ('Validation Accuracy', pipeline.score(X_validate_imputed, y_validate))\n",
    "\n",
    "\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = range(400, 1000, 100)\n",
    "criterion = ['entropy']\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = [4]\n",
    "min_samples_split = [8]\n",
    "min_samples_leaf = [10]\n",
    "random_state = range(1, 42, 1)\n",
    "bootstrap = [True]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'criterion' : criterion,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'random_state' : random_state,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random02 = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 1000, cv = 3, verbose=2, random_state=42, n_jobs=None)\n",
    "# Fit the random search model\n",
    "rf_random02.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.77 µs\n",
      "Baseline RandomForestClassifier Training accuracy: 0.9979166666666667\n",
      "Baseline RandomForestClassifier Validation accuracy: 0.8113636363636364\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(strategy=\"mean\"),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-2))\n",
    "    \n",
    "%time\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Baseline RandomForestClassifier Training accuracy:\", pipeline.score(X_train, y_train))\n",
    "print(\"Baseline RandomForestClassifier Validation accuracy:\", pipeline.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n",
      "RandomForestClassifier. n_estimators=200 Training accuracy: 0.9999789562289563\n",
      "RandomForestClassifier, n_estimators=200 Validation accuracy: 0.8104377104377104\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(strategy=\"mean\"),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-2, n_estimators=200))\n",
    "    \n",
    "%time\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"RandomForestClassifier. n_estimators=200 Training accuracy:\", pipeline.score(X_train, y_train))\n",
    "print(\"RandomForestClassifier, n_estimators=200 Validation accuracy:\", pipeline.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 4.77 µs\n",
      "RandomForestClassifier. n_estimators=1000 Training accuracy: 0.9979377104377104\n",
      "RandomForestClassifier, n_estimators=1000 Validation accuracy: 0.8116161616161616\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(strategy=\"mean\"),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-2, n_estimators=1000))\n",
    "    \n",
    "%time\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"RandomForestClassifier. n_estimators=1000 Training accuracy:\", pipeline.score(X_train, y_train))\n",
    "print(\"RandomForestClassifier, n_estimators=1000 Validation accuracy:\", pipeline.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 9.78 µs\n",
      "RandomForestClassifier. n_estimators=1000 min_samples_leaf=2 Training accuracy: 0.9441077441077441\n",
      "RandomForestClassifier, n_estimators=1000 min_samples_leaf=2 Validation accuracy: 0.8127104377104377\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(strategy=\"mean\"),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-2, n_estimators=1000, min_samples_leaf=2))\n",
    "    \n",
    "%time\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"RandomForestClassifier. n_estimators=1000 min_samples_leaf=2 Training accuracy:\", pipeline.score(X_train, y_train))\n",
    "print(\"RandomForestClassifier, n_estimators=1000 min_samples_leaf=2 Validation accuracy:\", pipeline.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 4.53 µs\n",
      "RandomForestClassifier. n_estimators=1000 min_samples_leaf=2 max_depth=32 Training accuracy: 0.9232954545454546\n",
      "RandomForestClassifier, n_estimators=1000 min_samples_leaf=2 max_depth=32 Validation accuracy: 0.8167508417508418\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(strategy=\"mean\"),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-2, n_estimators=1000, min_samples_leaf=2, max_depth=32))\n",
    "    \n",
    "%time\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"RandomForestClassifier. n_estimators=1000 min_samples_leaf=2 max_depth=32 Training accuracy:\", pipeline.score(X_train, y_train))\n",
    "print(\"RandomForestClassifier, n_estimators=1000 min_samples_leaf=2 max_depth=32 Validation accuracy:\", pipeline.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.48 µs\n",
      "RandomForestClassifier. n_estimators=1000 min_samples_leaf=2 max_depth=32 Training accuracy: 0.9436868686868687\n",
      "RandomForestClassifier, n_estimators=1000 min_samples_leaf=2 max_depth=32 Validation accuracy: 0.8141414141414142\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-2, n_estimators=1000, min_samples_leaf=2, max_depth=32))\n",
    "    \n",
    "%time\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"RandomForestClassifier. n_estimators=1000 min_samples_leaf=2 max_depth=32 Training accuracy:\", pipeline.score(X_train, y_train))\n",
    "print(\"RandomForestClassifier, n_estimators=1000 min_samples_leaf=2 max_depth=32 Validation accuracy:\", pipeline.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.25 µs\n",
      "RandomForestClassifier. n_estimators=1000 min_samples_leaf=2 max_depth=32 IterativeImputer Training accuracy: 0.9489898989898989\n",
      "RandomForestClassifier, n_estimators=1000 min_samples_leaf=2 max_depth=32 IterativeImputer Validation accuracy: 0.813973063973064\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    IterativeImputer(),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-2, n_estimators=1000, min_samples_leaf=2, max_depth=32))\n",
    "    \n",
    "%time\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"RandomForestClassifier. n_estimators=1000 min_samples_leaf=2 max_depth=32 IterativeImputer Training accuracy:\", pipeline.score(X_train, y_train))\n",
    "print(\"RandomForestClassifier, n_estimators=1000 min_samples_leaf=2 max_depth=32 IterativeImputer Validation accuracy:\", pipeline.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.48 µs\n",
      "RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20 Training accuracy: 0.9719486531986532\n",
      "RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20 Validation accuracy: 0.8088383838383838\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-2, max_features=.657, n_estimators=375, max_depth=20))\n",
    "    \n",
    "%time\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20 Training accuracy:\", pipeline.score(X_train, y_train))\n",
    "print(\"RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20 Validation accuracy:\", pipeline.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 9.54 µs\n",
      "RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Training accuracy: 0.9449074074074074\n",
      "RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Validation accuracy: 0.8118686868686869\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-2, max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2))\n",
    "    \n",
    "%time\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Training accuracy:\", pipeline.score(X_train, y_train))\n",
    "print(\"RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Validation accuracy:\", pipeline.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n",
      "RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Training accuracy: 0.9407196969696969\n",
      "RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Validation accuracy: 0.8121212121212121\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-2, max_features=.4, n_estimators=400, max_depth=20, min_samples_leaf=2))\n",
    "    \n",
    "%time\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Training accuracy:\", pipeline.score(X_train, y_train))\n",
    "print(\"RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Validation accuracy:\", pipeline.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 13.1 µs\n",
      "RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Training accuracy: 0.9357744107744108\n",
      "RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Validation accuracy: 0.812962962962963\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-2, max_features=.3, n_estimators=400, max_depth=20, min_samples_leaf=2))\n",
    "    \n",
    "%time\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Training accuracy:\", pipeline.score(X_train, y_train))\n",
    "print(\"RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Validation accuracy:\", pipeline.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.72 µs\n",
      "RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Training accuracy: 0.9287247474747474\n",
      "RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Validation accuracy: 0.8132996632996633\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-2, max_features=.2, n_estimators=400, max_depth=20, min_samples_leaf=2))\n",
    "    \n",
    "%time\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Training accuracy:\", pipeline.score(X_train, y_train))\n",
    "print(\"RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Validation accuracy:\", pipeline.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 11.7 µs\n",
      "RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Training accuracy: 0.929040404040404\n",
      "RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Validation accuracy: 0.813973063973064\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-2, max_features=.2, n_estimators=1000, max_depth=20, min_samples_leaf=2))\n",
    "    \n",
    "%time\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Training accuracy:\", pipeline.score(X_train, y_train))\n",
    "print(\"RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Validation accuracy:\", pipeline.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n",
      "RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Training accuracy: 0.9543981481481482\n",
      "RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Validation accuracy: 0.815993265993266\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-2, max_features=.2, n_estimators=1000, max_depth=30, min_samples_leaf=2))\n",
    "    \n",
    "%time\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Training accuracy:\", pipeline.score(X_train, y_train))\n",
    "print(\"RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Validation accuracy:\", pipeline.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.77 µs\n",
      "RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Training accuracy: 0.927946127946128\n",
      "RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Validation accuracy: 0.8165824915824916\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-2, max_features=.2, n_estimators=1000, max_depth=32, min_samples_leaf=2))\n",
    "    \n",
    "%time\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"RandomForestClassifier. max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Training accuracy:\", pipeline.score(X_train, y_train))\n",
    "print(\"RandomForestClassifier, max_features=.657, n_estimators=375, max_depth=20, min_samples_leaf=2 Validation accuracy:\", pipeline.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.TargetEncoder(min_samples_leaf=10, smoothing=50) # Both parameters > 1 to avoid overfitting\n",
    "X_train_encoded = encoder.fit_transform(X_train, y_train=='functional')\n",
    "X_validate_encoded = encoder.transform(X_validate, y_validate=='functional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed = SimpleImputer().fit_transform(X_train_encoded)\n",
    "X_validate_imputed = SimpleImputer().fit_transform(X_validate_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model=xgb.XGBClassifier(random_state=1,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.OrdinalEncoder()\n",
    "X_train_encoded = encoder.fit_transform(X_train, y_train)\n",
    "X_validate_encoded = encoder.fit_transform(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed = SimpleImputer().fit_transform(X_train_encoded)\n",
    "X_validate_imputed = SimpleImputer().fit_transform(X_validate_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7184132996632997\n",
      "0.7187710437710437\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_imputed, y_train)\n",
    "\n",
    "print(model.score(X_train_imputed, y_train))\n",
    "\n",
    "print(model.score(X_validate_imputed, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7482954545454545\n",
      "0.7457070707070707\n"
     ]
    }
   ],
   "source": [
    "model=xgb.XGBClassifier(random_state=1,learning_rate=0.1)\n",
    "\n",
    "model.fit(X_train_imputed, y_train)\n",
    "\n",
    "print(model.score(X_train_imputed, y_train))\n",
    "\n",
    "print(model.score(X_validate_imputed, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8081018518518519\n",
      "0.77996632996633\n"
     ]
    }
   ],
   "source": [
    "model=xgb.XGBClassifier(random_state=1,learning_rate=1)\n",
    "\n",
    "model.fit(X_train_imputed, y_train)\n",
    "\n",
    "print(model.score(X_train_imputed, y_train))\n",
    "\n",
    "print(model.score(X_validate_imputed, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8076809764309765\n",
      "0.7828282828282829\n"
     ]
    }
   ],
   "source": [
    "model=xgb.XGBClassifier(random_state=1,learning_rate=1, min_child_weight=2)\n",
    "\n",
    "model.fit(X_train_imputed, y_train)\n",
    "\n",
    "print(model.score(X_train_imputed, y_train))\n",
    "\n",
    "print(model.score(X_validate_imputed, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9172558922558922\n",
      "0.8163299663299664\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=32, max_features='auto', max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=2, min_samples_split=5,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
    "                       n_jobs=-1, oob_score=True, random_state=42, verbose=0,\n",
    "                       warm_start=False)\n",
    "\n",
    "model.fit(X_train_imputed, y_train)\n",
    "\n",
    "print(model.score(X_train_imputed, y_train))\n",
    "\n",
    "print(model.score(X_validate_imputed, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   37.9s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   38.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:   38.6s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:   38.6s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   38.6s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   39.1s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:   39.3s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:   39.3s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:   39.3s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   39.5s\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:   40.0s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   40.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   40.2s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   40.3s\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:  1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/geoprocessing/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:  2.2min\n"
     ]
    }
   ],
   "source": [
    "param_distributions = {\n",
    "    'n_estimators': [100, 250, 500, 1000],\n",
    "    'max_depth': [26, 32, 36, 40],\n",
    "    'min_samples_split': [1,3,5],\n",
    "    'min_samples_leaf': [1,2,3,4,5],\n",
    "    'max_features': [.1,.2,.3,.4,.5,.6,.7, .8,.9,1]\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1, random_state=8)\n",
    "\n",
    "final_search = RandomizedSearchCV(\n",
    "    estimator = rfc, \n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    cv=10,\n",
    "    verbose=50,\n",
    "    return_train_score=True,\n",
    "    random_state=8\n",
    ")\n",
    "\n",
    "final_search.fit(X_train_imputed, y_train)\n",
    "\n",
    "print(final_search.score(X_train_imputed, y_train))\n",
    "\n",
    "print(final_search.score(X_validate_imputed, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['amount_tsh', 'funder', 'gps_height', 'installer', 'longitude',\n",
       "       'latitude', 'wpt_name', 'num_private', 'basin', 'subvillage', 'region',\n",
       "       'region_code', 'district_code', 'lga', 'ward', 'population',\n",
       "       'public_meeting', 'scheme_management', 'scheme_name', 'permit',\n",
       "       'construction_year', 'extraction_type', 'extraction_type_group',\n",
       "       'extraction_type_class', 'management', 'management_group', 'payment',\n",
       "       'water_quality', 'quality_group', 'quantity', 'source', 'source_type',\n",
       "       'source_class', 'waterpoint_type', 'waterpoint_type_group',\n",
       "       'status_group', 'longitude_MISSING', 'latitude_MISSING',\n",
       "       'construction_year_MISSING', 'gps_height_MISSING', 'population_MISSING',\n",
       "       'year_recorded', 'month_recorded', 'day_recorded', 'years',\n",
       "       'years_MISSING'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43360     NaN\n",
       "7263      3.0\n",
       "2486      1.0\n",
       "313       NaN\n",
       "52726     NaN\n",
       "         ... \n",
       "9795      2.0\n",
       "58170     NaN\n",
       "17191     1.0\n",
       "8192     25.0\n",
       "49783    29.0\n",
       "Name: years, Length: 47520, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"years\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>status_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50785</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51630</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17168</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45559</td>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49871</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14353</th>\n",
       "      <td>39307</td>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14354</th>\n",
       "      <td>18990</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14355</th>\n",
       "      <td>28749</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14356</th>\n",
       "      <td>33492</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14357</th>\n",
       "      <td>68707</td>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14358 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    status_group\n",
       "0      50785      functional\n",
       "1      51630      functional\n",
       "2      17168      functional\n",
       "3      45559  non functional\n",
       "4      49871      functional\n",
       "...      ...             ...\n",
       "14353  39307  non functional\n",
       "14354  18990      functional\n",
       "14355  28749      functional\n",
       "14356  33492      functional\n",
       "14357  68707  non functional\n",
       "\n",
       "[14358 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "submission = sample_submission.copy()\n",
    "submission['status_group'] = y_pred\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('alex-pakalniskis-kaggle-submission-day-2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
